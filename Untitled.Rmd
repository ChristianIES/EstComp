---
title: "Aplicación de HMM"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(datasets)
```

### 1.- Importar set de datos y realizar Análisis Exploratorio.

Se importa el set de datos, seleccionando el rango de cantidad de caracteres a trabajar (114 a 170 en este experimento), para luego efectuar boxplot comparativo:

```{r b1, echo=TRUE}
dat=read.csv("SMSSpamCollection(cod3).csv",sep=";")
head(dat)
names(dat)

#largo entre 114 y 170 caracteres: muestra total = 1290
dat=dat[dat$largo>=114 & dat$largo<=170,]
dim(dat)

#AED cantidad de caracteres del mensaje:
library("ggpubr")
str(dat)
dat$resultado=as.factor(dat$resultado)
ggboxplot(dat, x = "resultado", y = "largo", 
          color = "resultado", palette = c("#E7B800", "#FC4E07"),
          order = c("spam","ham"),
          ylab = "Cantidad de Caracteres (largo)", xlab = "Spam/Ham")
```

Se estudia existencia de diferencias significativas, empleando prueba no paramétrica de Wilcoxon, que compara medianas. Esto debido a que, como se puede ver a través del test de Lilliefors implementado, la variable numérica en ninguno de los grupos puede describirse mediante una distribución Normal.

```{r b2, echo=TRUE}
#existencia de diferencias significativas:
table(dat$resultado)
library(nortest)
lillie.test(dat$largo[dat$resultado=="ham"])    #rechaza.
lillie.test(dat$largo[dat$resultado=="spam"])   #rechaza.

wilcox.test(dat$largo~dat$resultado)            #rechaza igualdad de medianas. Existen diferencias significativas.

```

### 2.- Naive Bayes

Se separa la muestra en training y test (70% - 30%), respetando secuencialidad para aplicar HMM. Luego se observan los resultados de la técnica de clasificación de Naive Bayes, a través de su matriz de confusión.

La implementación de Naive Bayes se encuentra disponible en la librería *e1071*.

```{r b3, echo=TRUE}
#division de la muestra en training y test:
dim(dat)
sample2=rep(c(TRUE, FALSE), c(913,1290-913))
entrenamiento2=dat[sample2, ]
prueba2=dat[!sample2, ]

#comparaciones de las bases:
table(entrenamiento2$resultado)/nrow(entrenamiento2)
table(prueba2$resultado)/nrow(prueba2)

#Naive Bayes:
#install.packages("e1071")
#install.packages("caTools")
#install.packages("caret")

library(e1071)
library(caTools)
library(caret)

nb2 = naiveBayes(resultado ~ largo, data = entrenamiento2)
nb2

#prediccion
prediccion2=predict(nb2, newdata = prueba2)

#matriz de confusion
mc2=table(prueba2$resultado, prediccion2)
mc2

#evaluacion general
confusionMatrix(mc2)
```

### 3.- Regresión Logística

Con la muestra separada en training y test (70% - 30%), se observan los resultados de la técnica de clasificación de Regresión Logística, a través de su matriz de confusión. El formato del resultado final (matriz de confusión) puede diferir, de modo que en la construcción de la tabla de métricas, presente en el reporte y presentación, se debe calcular poniendo atención a qué celdas corresponden a TP, FP, TN y FN.

En este caso, la categoría "spam" corresponde a 1 y "ham" (mensajes legítimos), a 0.

```{r b4, echo=TRUE}
str(entrenamiento2)
entrenamiento2$resultado2=ifelse(entrenamiento2$resultado=="spam",1,0)
entrenamiento2$resultado2=factor(entrenamiento2$resultado2)
modeloRL2=glm(resultado2~largo, family=binomial(link="logit"), data=entrenamiento2)
summary(modeloRL2)

library(caret)
prueba2$resultado2=ifelse(prueba2$resultado=="spam",1,0)
prueba2$resultado2=factor(prueba2$resultado2)
phat2=predict(modeloRL2, newdata=prueba2, type="response")
veredicto2 = ifelse(phat2 > 0.5, 1, 0)
veredicto2 = factor(veredicto2, levels=levels(prueba2$resultado2))
confusionMatrix(veredicto2,prueba2$resultado2)
```


### 4.- HMM

En primer lugar, se implementan los algoritmos de acuerdo con lo presentado por Coelho et al. (2019). En esta referencia se pueden encontrar códigos propuestos para Matlab.

Cabe señalar que la implementación base que presenta el texto para el algoritmo de Baum-Welch exhibirá inestabilidad numérica (se deben normalizar algunas probabilidades involucradas). No obstante, esto no sucederá con el algoritmo de Viterbi para el problema de decodificación (clasificación), ya que la normalización requerida fue incorporada.

```{r b6, echo=TRUE}
#1.- Forward Algorithm
#---------------------

forward=function(A,B,O,c){
  #m estados ocultos, n estados observables, N observaciones
  #A: mxm (matriz de transicion)
  #B: mxn (matriz de emisiones)
  #O: 1xN (vector de observaciones)
  #c: 1xm (vector de probabilidades iniciales)
  
  m=nrow(B)
  n=ncol(B)
  N=length(O)
  Alpha=matrix(0,nrow=N,ncol=m)
  
  #Inicializacion:
  for (k in 1:m){
    Alpha[1,k]=c[k]*B[k,O[1]]   
    #Nota: requiere que los estados observados esten relacionados
    #en orden con las columnas de la matriz B.
  }
  
  #Recursion:
  for (l in 2:N){
    for (k in 1:m){
      S=0
      for (i in 1:m){
        S=S+A[i,k]*Alpha[l-1,i]
      }
      Alpha[l,k]=B[k,O[l]]*S
    }
  }
  
  #Probabilidad de observar la secuencia:
  #Nota: en este caso interesa la ultima fila.
  P=sum(Alpha[N,])
  
  return(P)
}


#Variacion para que retorne la matriz Alpha:
#-------------------------------------------

forward_matriz=function(A,B,O,c){
  #m estados ocultos, n estados observables, N observaciones
  #A: mxm (matriz de transicion)
  #B: mxn (matriz de emisiones)
  #O: 1xN (vector de observaciones)
  #c: 1xm (vector de probabilidades iniciales)
  
  m=nrow(B)
  n=ncol(B)
  N=length(O)
  Alpha=matrix(0,nrow=N,ncol=m)
  
  #Inicializacion:
  for (k in 1:m){
    Alpha[1,k]=c[k]*B[k,O[1]]   
    #Nota: requiere que los estados observados esten relacionados
    #en orden con las columnas de la matriz B.
  }
  
  #Recursion:
  for (l in 2:N){
    for (k in 1:m){
      S=0
      for (i in 1:m){
        S=S+A[i,k]*Alpha[l-1,i]
      }
      Alpha[l,k]=B[k,O[l]]*S
    }
  }
  
  
  return(Alpha)
}


#2.- Backward Algorithm
#----------------------

backward=function(A,B,O){
  #m estados ocultos, n estados observables, N observaciones
  #A: mxm (matriz de transicion)
  #B: mxn (matriz de emisiones)
  #O: 1xN (vector de observaciones)
  #c: 1xm (vector de probabilidades iniciales)
  
  m=nrow(B)
  n=ncol(B)
  N=length(O)
  Beta=matrix(0,nrow=N,ncol=m)
  
  #Inicializacion:
  for (k in 1:m){
    Beta[N,k]=1
    #Nota: corresponde a la condición de borde.
  }
  #p=1
  #print(p)
  
  #Recursion:
  for(t in 1:(N-1)){
    #print("hola")
    #print(t)
    for(i in 1:m){
      Beta[N-t,i]=0
      for(j in 1:m){
        #p=p+1
        #r=N-t+1
        #print(r)
        Beta[N-t,i]=Beta[N-t,i]+A[i,j]*B[j,O[N-t+1]]*Beta[(N-t+1),j]
        #print(p)
        }
    }
  }
  
  #Esta funcion retorna la matriz completa:
  return(Beta)
}

#3.- Viterbi Algorithm
#---------------------

viterbi=function(A,B,O,c){
  #m estados ocultos, n estados observables, N observaciones
  #A: mxm (matriz de transicion)
  #B: mxn (matriz de emisiones)
  #O: 1xN (vector de observaciones)
  #c: 1xm (vector de probabilidades iniciales)
  
  m=nrow(B)
  n=ncol(B)
  N=length(O)
  rho=matrix(0,nrow=N,ncol=m)
  rhoprima=rho
  psi=matrix(0,nrow=N,ncol=m)
  
  #Inicializacion:
  for (k in 1:m){
    #condicion de borde: no hay estados en momento anterior
    psi[1,k]=0
    #condicion de borde para rho:
    rho[1,k]=c[k]*B[k,O[1]]
  }
  
  #Normalizacion para prevenir numeric underflow:
  for (k in 1:m){
    rhoprima[1,k]=rho[1,k]/sum(rho[1,])
  }
  

  #Recursion: 
  temporal=seq(0,0,length.out = m)
  temporal2=seq(0,0,length.out = m)
  for(k in 2:N){
    for(j in 1:m){
      for(i in 1:m){
        temporal[i]=rho[k-1,i]*A[i,j]*B[j,O[k]]/sum(rho[k-1,])
        temporal2[i]=rho[k-1,i]*A[i,j]/sum(rho[k-1,])
      }
      maximo=max(temporal2)
      rho[k,j]=max(temporal)
      psi[k,j]=which.max(temporal2)
    }
  }
  
  #Path finding:
  q=seq(0,0,length.out=N)
  q[N]=which.max(rho[N,])
  for(k in 1:(N-1)){
    q[N-k]=psi[N-k+1,q[N-k+1]]
  }
  #print(rho)
  return(q)
}

#2.- Baum-Welch Algorithm
#------------------------

#a) funcion gama:
calcular_gama=function(alfa,beta){
  #alfa: producto de Forward
  #beta: producto de Backward
  N=nrow(alfa)
  m=ncol(alfa)
  gama=alfa
  for(i in 1:N){
    den=0
    for(j in 1:m){
      #gama[i,j]=sum(gama[1:i,j])
      den=den+alfa[i,j]*beta[i,j]
    }
    for(t in 1:m){
      gama[i,t]=alfa[i,t]*beta[i,t]/den
    }
  }
  return(gama)
}

#b) funcion nu:
calcular_nu=function(gama){
  #gama: matriz Nxm
  N=nrow(gama)
  m=ncol(gama)
  nu=gama
  #esta funcion realiza sumatorias sobre gama
  for(i in 1:N){
    for(j in 1:m){
      nu[i,j]=sum(gama[1:i,j])
    } 
  }
  return(nu)
}

#c)funcion chi:
calcular_chi=function(alfa,beta,A,B,k,O){
  #Alfa:  del algoritmo forward -> Nxm
  #Beta:  del algoritmo backward -> Nxm
  #A:     matriz de transicion a 1 periodo -> mxm 
  #B:     emisiones -> mxn
  #k:     instante en que se esta calculando chi, con k<=N
  #O:     vector de observaciones -> 1xN
  
  #devolvera una matriz mxm y representa una foto del instante k
  N=nrow(alfa)
  m=ncol(alfa)
  chi=A
  den=0
  for(i in 1:m){
    for(j in 1:m){
      den=den+alfa[k,i]*A[i,j]*B[i,O[k+1]]*beta[k+1,j]
    }
  }
  for(i in 1:m){
    for(j in 1:m){
      chi[i,j]=alfa[k,i]*A[i,j]*B[i,O[k+1]]*beta[k+1,j]/den
    }
  }
  return(chi)
}

#d)funcion tau:
calcular_tau=function(O,alfa,beta,A,B){
  #es una foto para un k fijo
  
  #O:   vector de observaciones -> 1xN
  
  m=nrow(A)
  N=length(O)
  tau=matrix(0,nrow=m,ncol=m)
  for(k in 1:(N-1)){
    mat=calcular_chi(alfa,beta,A,B,k,O)
    for(i in 1:m){
      for(j in 1:m){
        tau[i,j]=tau[i,j]+mat[i,j]
      }
    }
  }
  return(tau)
}

#d)funcion taui:
calcular_taui=function(gama){
  #gama:  de la funcion gama -> Nxm
  
  N=nrow(gama)
  m=ncol(gama)
  taui=seq(0,0,length.out = m)
  for(i in 1:m){
    taui[i]=sum(gama[1:(N-1),i])
  }
  return(taui)
}

#e)funcion omega:
calcular_omega=function(gama,O,B){
  #gama:  de la funcion gama -> Nxm
  #B:     emisiones -> mxn
  #O:     secuencia de observaciones -> 1xN
  #Omega: matriz de mxn (estados x observaciones)
  N=length(O)
  n=ncol(B)
  m=nrow(B)
  omega=matrix(0,nrow=m,ncol=n)
  #for(j in 1:n){
    #posiciones=which(O==j)
    # if(length(posiciones)>0){
    #   omega[,j]=sum(gama[posiciones,])
    # }else{
    #   omega[,j]=0
    # }
    
  #}
  for(i in 1:m){
    for(j in 1:n){
      for(k in 1:N){
        if(O[k]==j){
          omega[i,j]=omega[i,j]+gama[k,i]
        }else{
          omega[i,j]=omega[i,j]+0
        }
      }
    }
  }
  #print(omega)
  return(omega)
}

#f)Baum-Welch:
bw=function(A,B,O,c,n.iter){
  for(r in 1:n.iter){
  #el algoritmo recibe estimaciones iniciales
  m=nrow(A)
  N=length(O)
  n=ncol(B)
  alfa=forward_matriz(A,B,O,c)
  #print(alfa)
  beta=backward(A,B,O)
  #print(beta)
  gama=calcular_gama(alfa,beta)
  #print(gama)
  tau=calcular_tau(O,alfa,beta,A,B)
  #print(tau)
  taui=calcular_taui(gama)
  #print(taui)
  nu=calcular_nu(gama)
  #print(nu)
  omega=calcular_omega(gama,O,B)
  
  #salida de parametros
  c_gorro=gama[1,]
  #print(c_gorro)
  A_gorro=matrix(0,nrow=m,ncol=m)
  for(i in 1:m){
    for(j in 1:m){
      A_gorro[i,j]=tau[i,j]/taui[i]
    }
  }
  #print(A_gorro)
  B_gorro=matrix(0,nrow=m,ncol=n)
  for(i in 1:m){
    for(j in 1:n){
      #print(nu[N,i])
      B_gorro[i,j]=omega[i,j]/nu[N,i]
    }
  }
  #print(B_gorro)
  A=A_gorro
  B=B_gorro
  c=c_gorro
  }
  lista=list("Probabilidad inicial"=c_gorro,"Matriz de transicion"=A_gorro,"Matriz de emisiones"=B_gorro)
  return(lista)
}
```

Se divide la muestra en training y test y se estiman los valores iniciales para las matrices: $A$ (matriz de probabilidades de transición); $B$ (matriz de probabilidades de emisión); $c$ (ley de probabilidad inicial). En este punto, se recomienda consultar la notación disponible en el informe que acompaña a este estudio.

```{r b7, echo=TRUE}
#largo entre 114 y 170 caracteres:
dat=read.csv("SMSSpamCollection(cod3).csv",sep=";")
dat=dat[dat$largo>=114 & dat$largo<=170,]
dim(dat)

table(dat$resultado)/1290

#probabilidad inicial:
initial_distribution = c(0.52,0.48)

#matriz de probabilidades de transicion:
prop.table(table(dat$resultado[1:1289],dat$resultado[2:1290]),1)
a=matrix(c(0.5,0.5,0.54,0.46),nrow=2,ncol=2,byrow=T)
a

#matriz de emisiones:
dat$largo_cod=dat$largo-113
dat$resultado_cod=ifelse(dat$resultado=="ham",1,2)
b=prop.table(table(dat$resultado_cod,dat$largo_cod),1)
b=matrix(b,ncol=ncol(b))
b

#division de la muestra en training y test, asumiendo secuencialidad de los mensajes:
#proporciones: 70% vs 30% (entrenamiento vs prueba)
#------------------------------------------------------------------------------------

dim(dat)
sample3=rep(c(TRUE, FALSE), c(913,1290-913))
entrenamiento3=dat[sample3, ]
prueba3=dat[!sample3, ]
```


Para garantizar la estabilidad numérica, se iterará utilizando la implementación del algoritmo Baum-Welch disponible en R (librería *HMM*), mientras que el algoritmo de Viterbi se ejecutará a partir de la implementación realizada según las directrices de Coelho et al. (2019). Se obtiene la matriz de confusión, luego de decodificar, empleando los parámetros obtenidos al iterar con el algoritmo de Baum-Welch.

La nomenclatura en la matriz de confusión puede variar con respecto a los métodos anteriores, de modo que debe analizarse el resultado con precaución, para así efectuar los cálculos posteriores de métricas de desempeño de manera apropiada. Por ejemplo, en este caso los mensajes de la clase "ham" (válidos) corresponden a 1 y "spam", a 2.

```{r b8, echo=TRUE}
#prueba algoritmo Baum-Welch con libreria HMM:
#=============================================
library(HMM)
hmm = initHMM(c(1,2),1:57,transProbs=a,emissionProbs=b)
print(hmm)

bw = baumWelch(hmm,dat$largo_cod,20)
bw$hmm$startProbs
bw$hmm$transProbs
bw$hmm$emissionProbs

#decodificacion con algoritmo Viterbi:
#-------------------------------------
a_hat=matrix(bw$hmm$transProbs,nrow=2,ncol=2)
b_hat=matrix(bw$hmm$emissionProbs,nrow=2,ncol=57)
c_hat=matrix(bw$hmm$startProbs,nrow=1,ncol=2)

res=viterbi(a_hat,b_hat,prueba3$largo_cod,c_hat)
res

str(prueba3)
prueba3$resultado2=ifelse(prueba3$resultado=="ham",1,2)
prueba3$resultado2=as.factor(prueba3$resultado2)
res=as.factor(res)
confusionMatrix(res,prueba3$resultado2)
```


Se efectúa análisis de sensibilidad sobre la solución, específicamente sobre los resultados de la matriz de confusión, utilizando en el algoritmo de Viterbi las estimaciones iniciales de los parámetros.

Al igual que en el caso anterior, en la matriz de confusión los mensajes de la clase "ham" (válidos) corresponden a 1 y "spam", a 2.

```{r b9, echo=TRUE}
#decodificacion con algoritmo Viterbi (analisis de sensibilidad):
#----------------------------------------------------------------
res2=viterbi(a,b,prueba3$largo_cod,initial_distribution)
res2

str(prueba3)
prueba3$resultado2=ifelse(prueba3$resultado=="ham",1,2)
prueba3$resultado2=as.factor(prueba3$resultado2)
res2=as.factor(res2)
confusionMatrix(res2,prueba3$resultado2)
```


### 5.- Referencias

* Xia, T. y Chen, X. (2020). A Discrete Hidden Markov Model for SMS Spam Detection. MDPI. \emph{Applied Sciences,} 10(14). https://doi.org/10.3390/app10145011.


* Srivastava, A., Kundu, A., Sural, S. y Majumdar, A. (2008). Credit Card Fraud Detection Using Hidden Markov Model. IEEE. \emph{Transactions on Dependable and Secure Computing,} 5(1). https://doi.org/10.1109/TDSC.2007.70228.

* Witteveen, D. y Attewell, P. (2016). The College Completion Puzzle: A Hidden Markov Model Approach. \empf{Res High Educ}, 58, 449–467. https://doi.org/10.1007/s11162-016-9430-2.

* Coelho, J. P., Pinho T. M. y Boaventura-Cunha, J. (2019). \emph{Hidden Markov Models, Theory and Implementation using MATLAB.} CRC Press.

* Zucchini, W., MacDonald I. y Langrock, R. (2016). \emph{Hidden Markov Models for Time Series, An Introduction Using R.} CRC Press.

* UCI Machine Learning Repository, Center for Machine Learning and Intelligent Systems. (2012). \emph{SMS Spam Collection Data Set.} [Conjunto de Datos.] https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection#



